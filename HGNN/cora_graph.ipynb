{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dryrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def hypergraph_construction(H):\n",
    "    H = H.float()\n",
    "\n",
    "    Dv = torch.sum(H, dim=1)\n",
    "    De = torch.sum(H, dim=0)\n",
    "\n",
    "    # Fix: avoid division by zero\n",
    "    Dv = torch.where(Dv == 0, torch.ones_like(Dv), Dv)\n",
    "    De = torch.where(De == 0, torch.ones_like(De), De)\n",
    "\n",
    "    Dv_inv_sqrt = torch.diag(torch.pow(Dv, -0.5))\n",
    "    De_inv = torch.diag(torch.pow(De, -1.0))\n",
    "\n",
    "    HT = H.t()\n",
    "    G = Dv_inv_sqrt @ H @ De_inv @ HT @ Dv_inv_sqrt\n",
    "    return G\n",
    "\n",
    "class HGNNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(HGNNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias=False)\n",
    "\n",
    "    def forward(self, x, G):\n",
    "        return self.linear(G @ x)\n",
    "\n",
    "class HGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(HGNN, self).__init__()\n",
    "        self.layer1 = HGNNLayer(in_dim, hidden_dim)\n",
    "        self.layer2 = HGNNLayer(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, G):\n",
    "        x = F.relu(self.layer1(x, G))\n",
    "        x = self.layer2(x, G)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "def generate_planetoid_split(labels, num_classes=7, train_per_class=20, val_size=500, test_size=1000, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    labels = labels.cpu().numpy()\n",
    "    idx = np.arange(len(labels))\n",
    "\n",
    "    train_idx = []\n",
    "    for i in range(num_classes):\n",
    "        cls_idx = idx[labels == i]\n",
    "        train_idx.extend(np.random.choice(cls_idx, train_per_class, replace=False))\n",
    "\n",
    "    remaining = np.setdiff1d(idx, train_idx)\n",
    "    np.random.shuffle(remaining)\n",
    "\n",
    "    val_idx = remaining[:val_size]\n",
    "    test_idx = remaining[val_size:val_size + test_size]\n",
    "\n",
    "    return torch.LongTensor(train_idx), torch.LongTensor(val_idx), torch.LongTensor(test_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss: 1.9461 | Train Acc: 0.9857 | Val Acc: 0.6140 | Test Acc: 0.5980\n",
      "Epoch 010 | Loss: 1.3577 | Train Acc: 1.0000 | Val Acc: 0.7240 | Test Acc: 0.7080\n",
      "Epoch 020 | Loss: 0.3747 | Train Acc: 1.0000 | Val Acc: 0.8020 | Test Acc: 0.7650\n",
      "Epoch 030 | Loss: 0.0960 | Train Acc: 1.0000 | Val Acc: 0.7960 | Test Acc: 0.7730\n",
      "Epoch 040 | Loss: 0.0726 | Train Acc: 1.0000 | Val Acc: 0.8000 | Test Acc: 0.7870\n",
      "Epoch 050 | Loss: 0.0776 | Train Acc: 1.0000 | Val Acc: 0.8020 | Test Acc: 0.7940\n",
      "Epoch 060 | Loss: 0.0680 | Train Acc: 1.0000 | Val Acc: 0.8060 | Test Acc: 0.8010\n",
      "Epoch 070 | Loss: 0.0581 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8030\n",
      "Epoch 080 | Loss: 0.0535 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8050\n",
      "Epoch 090 | Loss: 0.0499 | Train Acc: 1.0000 | Val Acc: 0.8120 | Test Acc: 0.8040\n",
      "Epoch 100 | Loss: 0.0466 | Train Acc: 1.0000 | Val Acc: 0.8120 | Test Acc: 0.8030\n",
      "Epoch 110 | Loss: 0.0440 | Train Acc: 1.0000 | Val Acc: 0.8120 | Test Acc: 0.8030\n",
      "Epoch 120 | Loss: 0.0419 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8050\n",
      "Epoch 130 | Loss: 0.0401 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8070\n",
      "Epoch 140 | Loss: 0.0385 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8070\n",
      "Epoch 150 | Loss: 0.0372 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8080\n",
      "Epoch 160 | Loss: 0.0360 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8080\n",
      "Epoch 170 | Loss: 0.0350 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8080\n",
      "Epoch 180 | Loss: 0.0341 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8080\n",
      "Epoch 190 | Loss: 0.0333 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8070\n",
      "Epoch 200 | Loss: 0.0326 | Train Acc: 1.0000 | Val Acc: 0.8100 | Test Acc: 0.8070\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "# from hgnn_model import HGNN, hypergraph_construction\n",
    "# from utils import generate_planetoid_split\n",
    "\n",
    "# Load Cora (citation graph)\n",
    "data = Planetoid(root='/tmp/Cora', name='Cora')[0]\n",
    "features, labels = data.x, data.y\n",
    "edge_index = data.edge_index\n",
    "\n",
    "# Construct hyperedges: treat each undirected edge as a hyperedge\n",
    "edges = edge_index.t().tolist()\n",
    "edges = [tuple(sorted(e)) for e in edges]\n",
    "unique_edges = list(set(edges))\n",
    "\n",
    "num_nodes = features.shape[0]\n",
    "num_edges = len(unique_edges)\n",
    "H = torch.zeros((num_nodes, num_edges))\n",
    "for j, (u, v) in enumerate(unique_edges):\n",
    "    H[u, j] = 1.0\n",
    "    H[v, j] = 1.0\n",
    "\n",
    "# Normalize features\n",
    "features[torch.isnan(features)] = 0\n",
    "features[torch.isinf(features)] = 0\n",
    "features = F.normalize(features, p=2, dim=1)\n",
    "\n",
    "# Build G\n",
    "G = hypergraph_construction(H)\n",
    "\n",
    "# Split\n",
    "train_idx, val_idx, test_idx = generate_planetoid_split(labels)\n",
    "\n",
    "# Model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "features, labels, G = features.to(device), labels.to(device), G.to(device)\n",
    "train_idx, val_idx, test_idx = train_idx.to(device), val_idx.to(device), test_idx.to(device)\n",
    "\n",
    "model = HGNN(in_dim=features.shape[1], hidden_dim=64, out_dim=labels.max().item() + 1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# Training loop\n",
    "def evaluate(model, x, G, labels, idx):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x, G)\n",
    "        pred = out[idx].argmax(dim=1)\n",
    "        acc = (pred == labels[idx]).float().mean()\n",
    "    return acc.item()\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    model.train()\n",
    "    out = model(features, G)\n",
    "    loss = F.cross_entropy(out[train_idx], labels[train_idx])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_acc = evaluate(model, features, G, labels, train_idx)\n",
    "    val_acc = evaluate(model, features, G, labels, val_idx)\n",
    "    test_acc = evaluate(model, features, G, labels, test_idx)\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | Loss: {loss:.4f} | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f} | Test Acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.datasets import Planetoid\n",
    "# from hgnn_model import HGNN, hypergraph_construction\n",
    "# from utils import generate_planetoid_split\n",
    "import time\n",
    "\n",
    "def graph_to_hypergraph(edge_index, num_nodes):\n",
    "    edges = edge_index.t().tolist()\n",
    "    edges = [tuple(sorted(e)) for e in edges]\n",
    "    unique_edges = list(set(edges))\n",
    "\n",
    "    H = torch.zeros((num_nodes, len(unique_edges)))\n",
    "    for j, (u, v) in enumerate(unique_edges):\n",
    "        H[u, j] = 1.0\n",
    "        H[v, j] = 1.0\n",
    "    return H\n",
    "\n",
    "def run_hgnn_on(dataset_name, hidden_dim=128, epochs=200, lr=0.003, weight_decay=5e-4, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = Planetoid(root=f'/tmp/{dataset_name}', name=dataset_name)\n",
    "    data = dataset[0]\n",
    "    features, labels = data.x, data.y\n",
    "    edge_index = data.edge_index\n",
    "\n",
    "    # Build hypergraph from citation edges\n",
    "    H = graph_to_hypergraph(edge_index, features.shape[0])\n",
    "    G = hypergraph_construction(H)\n",
    "\n",
    "    # Normalize features\n",
    "    features[torch.isnan(features)] = 0\n",
    "    features[torch.isinf(features)] = 0\n",
    "    features = F.normalize(features, p=2, dim=1)\n",
    "\n",
    "    # Splits\n",
    "    train_idx, val_idx, test_idx = generate_planetoid_split(labels, num_classes=labels.max().item()+1)\n",
    "\n",
    "    # To device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    features, labels, G = features.to(device), labels.to(device), G.to(device)\n",
    "    train_idx, val_idx, test_idx = train_idx.to(device), val_idx.to(device), test_idx.to(device)\n",
    "\n",
    "    # Model\n",
    "    model = HGNN(in_dim=features.shape[1], hidden_dim=hidden_dim, out_dim=labels.max().item() + 1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    acc_list = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        out = model(features, G)\n",
    "        loss = F.cross_entropy(out[train_idx], labels[train_idx])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(features, G)\n",
    "            pred = logits[test_idx].argmax(dim=1)\n",
    "            acc = (pred == labels[test_idx]).float().mean().item()\n",
    "            acc_list.append(acc)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == 1 or epoch == epochs:\n",
    "            print(f\"[{dataset_name}] Epoch {epoch:03d} | Loss: {loss:.4f} | Test Acc: {acc:.4f}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    return acc_list, total_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Cora] Epoch 001 | Loss: 1.9457 | Test Acc: 0.5200\n",
      "[Cora] Epoch 010 | Loss: 1.7500 | Test Acc: 0.7130\n",
      "[Cora] Epoch 020 | Loss: 1.3265 | Test Acc: 0.7290\n",
      "[Cora] Epoch 030 | Loss: 0.7966 | Test Acc: 0.7470\n",
      "[Cora] Epoch 040 | Loss: 0.3995 | Test Acc: 0.7620\n",
      "[Cora] Epoch 050 | Loss: 0.2139 | Test Acc: 0.7720\n",
      "[Cora] Epoch 060 | Loss: 0.1462 | Test Acc: 0.7730\n",
      "[Cora] Epoch 070 | Loss: 0.1225 | Test Acc: 0.7800\n",
      "[Cora] Epoch 080 | Loss: 0.1111 | Test Acc: 0.7850\n",
      "[Cora] Epoch 090 | Loss: 0.1017 | Test Acc: 0.7900\n",
      "[Cora] Epoch 100 | Loss: 0.0929 | Test Acc: 0.7970\n",
      "[Cora] Epoch 110 | Loss: 0.0853 | Test Acc: 0.7990\n",
      "[Cora] Epoch 120 | Loss: 0.0792 | Test Acc: 0.8030\n",
      "[Cora] Epoch 130 | Loss: 0.0742 | Test Acc: 0.8050\n",
      "[Cora] Epoch 140 | Loss: 0.0700 | Test Acc: 0.8050\n",
      "[Cora] Epoch 150 | Loss: 0.0664 | Test Acc: 0.8060\n",
      "[Cora] Epoch 160 | Loss: 0.0633 | Test Acc: 0.8050\n",
      "[Cora] Epoch 170 | Loss: 0.0605 | Test Acc: 0.8050\n",
      "[Cora] Epoch 180 | Loss: 0.0580 | Test Acc: 0.8080\n",
      "[Cora] Epoch 190 | Loss: 0.0558 | Test Acc: 0.8090\n",
      "[Cora] Epoch 200 | Loss: 0.0538 | Test Acc: 0.8090\n",
      "[Citeseer] Epoch 001 | Loss: 1.7914 | Test Acc: 0.5860\n",
      "[Citeseer] Epoch 010 | Loss: 1.4806 | Test Acc: 0.6200\n",
      "[Citeseer] Epoch 020 | Loss: 0.9010 | Test Acc: 0.6320\n",
      "[Citeseer] Epoch 030 | Loss: 0.4199 | Test Acc: 0.6760\n",
      "[Citeseer] Epoch 040 | Loss: 0.2110 | Test Acc: 0.6940\n",
      "[Citeseer] Epoch 050 | Loss: 0.1482 | Test Acc: 0.6910\n",
      "[Citeseer] Epoch 060 | Loss: 0.1307 | Test Acc: 0.6920\n",
      "[Citeseer] Epoch 070 | Loss: 0.1203 | Test Acc: 0.6850\n",
      "[Citeseer] Epoch 080 | Loss: 0.1091 | Test Acc: 0.6850\n",
      "[Citeseer] Epoch 090 | Loss: 0.0995 | Test Acc: 0.6830\n",
      "[Citeseer] Epoch 100 | Loss: 0.0924 | Test Acc: 0.6860\n",
      "[Citeseer] Epoch 110 | Loss: 0.0868 | Test Acc: 0.6840\n",
      "[Citeseer] Epoch 120 | Loss: 0.0822 | Test Acc: 0.6820\n",
      "[Citeseer] Epoch 130 | Loss: 0.0782 | Test Acc: 0.6830\n",
      "[Citeseer] Epoch 140 | Loss: 0.0748 | Test Acc: 0.6820\n",
      "[Citeseer] Epoch 150 | Loss: 0.0719 | Test Acc: 0.6820\n",
      "[Citeseer] Epoch 160 | Loss: 0.0693 | Test Acc: 0.6800\n",
      "[Citeseer] Epoch 170 | Loss: 0.0670 | Test Acc: 0.6790\n",
      "[Citeseer] Epoch 180 | Loss: 0.0650 | Test Acc: 0.6770\n",
      "[Citeseer] Epoch 190 | Loss: 0.0631 | Test Acc: 0.6770\n",
      "[Citeseer] Epoch 200 | Loss: 0.0615 | Test Acc: 0.6770\n",
      "[PubMed] Epoch 001 | Loss: 1.0988 | Test Acc: 0.6920\n",
      "[PubMed] Epoch 010 | Loss: 0.9538 | Test Acc: 0.7280\n",
      "[PubMed] Epoch 020 | Loss: 0.6669 | Test Acc: 0.7360\n",
      "[PubMed] Epoch 030 | Loss: 0.3767 | Test Acc: 0.7660\n",
      "[PubMed] Epoch 040 | Loss: 0.1878 | Test Acc: 0.7660\n",
      "[PubMed] Epoch 050 | Loss: 0.1007 | Test Acc: 0.7700\n",
      "[PubMed] Epoch 060 | Loss: 0.0682 | Test Acc: 0.7670\n",
      "[PubMed] Epoch 070 | Loss: 0.0573 | Test Acc: 0.7720\n",
      "[PubMed] Epoch 080 | Loss: 0.0536 | Test Acc: 0.7710\n",
      "[PubMed] Epoch 090 | Loss: 0.0512 | Test Acc: 0.7720\n",
      "[PubMed] Epoch 100 | Loss: 0.0482 | Test Acc: 0.7710\n",
      "[PubMed] Epoch 110 | Loss: 0.0449 | Test Acc: 0.7750\n",
      "[PubMed] Epoch 120 | Loss: 0.0418 | Test Acc: 0.7750\n",
      "[PubMed] Epoch 130 | Loss: 0.0393 | Test Acc: 0.7750\n",
      "[PubMed] Epoch 140 | Loss: 0.0372 | Test Acc: 0.7730\n",
      "[PubMed] Epoch 150 | Loss: 0.0354 | Test Acc: 0.7750\n",
      "[PubMed] Epoch 160 | Loss: 0.0339 | Test Acc: 0.7750\n",
      "[PubMed] Epoch 170 | Loss: 0.0325 | Test Acc: 0.7760\n",
      "[PubMed] Epoch 180 | Loss: 0.0312 | Test Acc: 0.7750\n",
      "[PubMed] Epoch 190 | Loss: 0.0301 | Test Acc: 0.7760\n",
      "[PubMed] Epoch 200 | Loss: 0.0291 | Test Acc: 0.7760\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"Cora\", \"Citeseer\", \"PubMed\"]\n",
    "results = {}\n",
    "\n",
    "for name in datasets:\n",
    "    acc, time_taken = run_hgnn_on(name)\n",
    "    results[name] = {\"acc\": acc, \"time\": time_taken}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset    | Max Accuracy | Time Taken (s) \n",
      "---------------------------------------------\n",
      "Cora       |      80.90%     |         0.97 sec\n",
      "Citeseer   |      69.50%     |         2.91 sec\n",
      "PubMed     |      77.70%     |        16.51 sec\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Dataset':<10} | {'Max Accuracy':<12} | {'Time Taken (s)':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for name in ['Cora', 'Citeseer', 'PubMed']:\n",
    "    acc = max(results[name]['acc'])\n",
    "    time_taken = results[name]['time']\n",
    "    print(f\"{name:<10} | {acc*100:>10.2f}%     | {time_taken:>12.2f} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ridham.patel/hypergraph-baselines-2/HGNN\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gntk-gdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
